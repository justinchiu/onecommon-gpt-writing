% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% New packages
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{mystyle}
\usepackage{multirow}

\newcommand{\system}{RPW}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{tabu}
\newsavebox{\DialogueBox}

\newenvironment{Dialogue}[1][\small]{
    #1
    \def\arraystretch{1.3}
    \setlength\tabcolsep{7pt}
    \taburulecolor{lightgray}
    \vspace{.8em}
    \noindent
    \begin{tabu} to \columnwidth {|[2pt]lX}
}{
    \end{tabu}
    \vspace{.5em}
}

\newcommand{\Partner}[1]{Partner: & \UserUtt{#1} \\}
\newcommand{\AgentDo}[1]{
\\[-1.3em]
\multicolumn{2}{|[2pt]p{\linewidth}}{ 
\AgentAction{#1}
}
\\[.3em]
}
\newcommand{\AgentSay}[1]{Agent: & \AgentUtt{#1} \\}
\newcommand{\UserUtt}[1]{\textit{#1}}
\newcommand{\AgentUtt}[1]{\textit{#1}}
\newcommand{\AgentAction}[1]{\texttt{\small #1}}
\newcommand{\MetaAction}[1]{\texttt{\small \underline{#1}}}
\newcommand{\Exec}{\MetaAction{\textcolor{red}{We don't use eval anymore!}}\xspace}
\newcommand{\Salient}{\MetaAction{refer}\xspace}
\newcommand{\Revise}{\MetaAction{revise}\xspace}
\newcommand{\ReviseConstraint}{\MetaAction{reviseConstraint}\xspace}
\newcommand{\Describe}{\MetaAction{describe}\xspace}
\newcommand{\scExec}{\texttt{\scriptsize\underline{eval}}\xspace}
\newcommand{\scSalient}{\texttt{\scriptsize\underline{refer}}\xspace}
\newcommand{\scRevise}{\texttt{\scriptsize\underline{revise}}\xspace}
\newcommand{\Recover}{\MetaAction{recover}\xspace}
\newcommand{\Root}{\textit{root}}
\newcommand{\OurDataset}{SMCalFlow\xspace}


\newcommand{\justin}[1]{{{\textcolor{purple}{(Justin: #1)}}}}
\newcommand{\derek}[1]{{{\textcolor{blue}{(Derek: #1)}}}}
\newcommand{\wenting}[1]{{{\textcolor{orange}{(Wenting: #1)}}}}
\newcommand{\daniel}[1]{{{\textcolor{brown}{(Daniel: #1)}}}}
\newcommand{\srush}[1]{{{\textcolor{green}{(Sasha: #1)}}}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{
Integrating Language Models and Symbolic Planning for Grounded Dialogue
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Justin T. Chiu  \\
Cornell Tech \\
\texttt{jtc257@cornell.edu}
\And
Wenting Zhao \\
Cornell University \\
\texttt{wz346@cornell.edu}
\And
Derek Chen \\
Columbia University \\
\texttt{dc3761@columbia.edu}
\And
Alexander M. Rush \\
Cornell Tech \\
\texttt{arush@cornell.edu}
\And
Daniel Fried \\
Carnegie Mellon University \\
\texttt{dfried@cs.cmu.edu} 
}

\begin{document}
\maketitle
\begin{abstract}
%Contributions:
%1. symbolic planning
%2. codegen for grounding
%3. results
Large language models excel at understanding and generating text and code; however,
they face limitations in adhering to task-specific constraints and handling novel grounding.
These limitations are exacerbated in grounded task-oriented dialogue,
where constraint and grounding errors compound.
We address these shortcomings by composing large language models
with a symbolic planner and code generation for grounding in a modular and interpretable dialogue system.
Our system consists of a reader, planner, and writer:
The reader leverages language models to convert partner utterances into executable code,
calling functions that perform grounding.
The translated code's output is stored to track dialogue state,
while a symbolic planner determines the next appropriate response.
We evaluate our system's performance on the demanding \textsc{OneCommon} dialogue task,
where dialogue is grounded on abstract images of scattered dots.
In this challenging setting, our system outperforms humans with a success rate of X\% vs 65.8\%.
\end{abstract}

\section{Introduction}
\begin{comment}
\justin{rewrite this, don't focus on OOD. code => step by step reasoning (vipergpt, CaP). extend to dialogue setting. dialogue challenges: ref res, belief, planning.}
Large language models are strong dialogue systems,
capable of obtaining reliable and accurate results in zero or few-shot settings \citep{fstod,lambda,godel}.
However, grounded dialogue often requires multimodal grounding in visual context.
Recent work composes large language models with separately pretrained vision models,
hoping that vision models can generalize to novel image domains \citep{blip2}.
This exposes grounded dialogue systems to a multiplicative source of error:
if just one of either the language or vision are out of domain for any component of a 
dialogue system, overall system accuracy will suffer.


One solution to compounding error from multimodality is modularity.
Recent work has investigated the use of code generating language models for composing vision and
language models in a modular fashion, processing each modality separately \citep{codeaspolicies2022,vipergpt}.
The modularity of these systems also enables task-specific biases to be built into systems.
For example, \citet{codeaspolicies2022} craft a Python library that serves as a tool a
code generation model can call in order to manipulate a robot arm.
\end{comment}

Grounded task-oriented dialogue requires adherence to task-specific constraints. \daniel{What are constraints? On actions taken, or on language used?}
Requires grounding and planning.

Fully end-to-end models, while extremely flexible, may fail to adhere to constraints,
and are not straightforward to control \citep{vipergpt}.

Recent work has proposed using code-generating language models as grounded question-answering systems
and robot policies,
demonstrating that code is a powerful medium for breaking down complex tasks into simple steps.

\begin{figure}[t!]
\setlength{\abovecaptionskip}{0pt}

\centering

\scalebox{1.1}{
\begin{tikzpicture}

%\filldraw[gray!40] (0,0) circle (.3em);
%\filldraw[gray!100] (.25,0) circle (.38em);
%\filldraw[gray!160] (.5,0) circle (.45em);

\filldraw[gray!160] (-.2,.6) circle (.45em); % 766
\filldraw[gray!40] (-.7,-.3) circle (.4em); % 51
\filldraw[gray!40] (.4,.1) circle (.3em); % 52
\filldraw[gray!100] (.8,-.1) circle (.3em); % 66

% left
\filldraw[gray!140] (-1.3,-1.2) circle (.3em); % 13
\filldraw[gray!100] (-1.6,-1.8) circle (.3em); % 14
\filldraw[gray!110] (-.2,-1.1) circle (.4em); % 74

% right
\filldraw[gray!100] (-1,1.5) circle (.3em); % 28
\filldraw[gray!100] (-.5,1.6) circle (.38em); % 69
\filldraw[gray!160] (.8,1.8) circle (.45em); % 26

% left selects 76
\draw[blue,thick] (-.2,.6) circle (.7em);
% right selects 26
\draw[red,thick] (.8,1.8) circle (.7em);

\draw[blue,thick] (-.3,-.7) circle (5em);
\draw[red,thick] (.2,1) circle (5em);
\end{tikzpicture}
}

\vspace{1em}

\small
\begin{tabular}[b]{@{}l@{}l@{}}
\toprule
{\color{blue} A}:\;&I have one large black dot by itself. Do you have it? \\
{\color{red} P}:\;&Yes, I do have that. \\
{\color{blue} A}:\;&Let's pick that one. \\
{\color{red} P}:\;&<select> {\color{red} red} \\
{\color{blue} A}:\;&<select> {\color{blue} blue}\\
\bottomrule
\end{tabular}

\vspace{1em}

\caption{
placeholder. replace with figure that shows value of planning
and codegen for grounding.
}
\label{fig:oc}
\end{figure}

Grounding with code-generating systems has largely been studied in the single turn setting \citep{vipergpt,codeaspolicies2022}.
\daniel{We should say \emph{why} code is useful for grounding, in a way that also covers OneCommon. e.g. breaking a complex task into reusable functions which can be executed in an environmental context?}
We present a system, \system{}, that extends minimally supervised code-generation for grounding
to the grounded dialogue setting by incorporating symbolic planning.
\system{} follows a classical dialogue system decomposition:
read, plan, and write \citep{young2013pomdpsurvey}.
The system reads partner utterances and parses them to code,
using the execution results to track dialogue state and produce a symbolic plan of what to say next.
%At the core of the system is the state representation: We integrate both a probabilistic belief state \citep{young2006pomdp} as well as code\wenting{integrate what in what?}.
%Our system does not require supervision beyond few-shot prompting,
%but may require a task-dependent library of code \wenting{can you be more positive and just say we only need minimum supervision (task-dependent library can just be regarded as instruction right? which every method needs)}.

\justin{mentioning python library is confusing}

\begin{comment}
rather than grounded dialogue \wenting{is this single turn vs multiple, or single turn vs grounded dialogue?}.
Grounded dialogue offers new challenges:
dialogue systems must deal with linguistic phenomena such as coreference and ellipsis,
state tracking, and planning.
We present a modular grounded dialogue system, \system{}, that utilizes code generation for 
grounding, while incorporating components of dialogue system pipelines for
handling the unique challenges afforded by dialogue \wenting{this part is a bit awkward in that it's too abstract for me to understand how code can handle coreference and ellipsis, state tracking, and planning all at the same time. Can you be more specific or just maybe focus on planning?}.
\end{comment}

We evaluate \system{} on \textsc{OneCommon}, a challenging grounded dialogue task where an agent and partner are given different but overlapping views and must find one dot in common, illustrated in Figure \ref{fig:oc} \citep{onecommon}.
The visual grounding in \textsc{OneCommon} is particularly challenging:
Players are required to discuss a set of abstract dots, a setting not commonly found in
pretraining datasets for vision models.
This is exacerbated by \textsc{OneCommon}'s partial observability;
players do not have exactly the same perspective,making mistakes a large risk.
\daniel{Need a very brief description of the setting in this paragraph -- overlapping views, each player sees one, need to agree. Refer to Fig 1.}
%The dots themselves are often difficult to describe \wenting{this is too specific, just say partial observability makes both parties harder to reach a common ground?}, with referring expressions made even more ambiguous by \textsc{OneCommon}'s partial observability.

In human evaluation, \system{} outperforms human players.
Demonstrates method for minimally supervised adaptation of dialogue systems to new grounded tasks.
\daniel{What's the minimal supervision -- is it writing functions for the code model to call, writing few shot prompts, both? And maybe ``construction... for'' rather than ``adaptation... to''}
\daniel{Make novel contribution more explicit, e.g. contrasting with prior work on code models for grounding which has all been single-turn}

\section{Related work}

\paragraph{Modular dialogue systems}
SHRDLU. POMDP Young \citep{young2006pomdp}. SMCalFlow \citep{sm}.
Decoupling strategy and generation \citep{he2018dnd}. \citep{fstod}.
Difference: Combine pieces of each: simple executable state,
flexibility afforded by code-generating language models,
probabilistic belief in grounded dialogue task.

\paragraph{Tool-based language models}
Code-as-policies \citep{codeaspolicies2022}, ViperGPT \citep{vipergpt},
PAL, toolformer, VisProg.
VisualChatGPT, chatgpt with plugins.
Difference: Explicit symbolic planning,
integration with classical dialogue system.

\paragraph{Planning}
Lots of recent LM planning papers.
\citep{gandhi2023strategic}
Task-oriented dialogue often uses multi-turn model-based planning.
Here planning is performed in the space of language via rollouts or tree-search
\citep{dnd,yarats2017rollout,ingress,jang2020bapomdp}.
%Our work reduces perspective-dependent ambiguity by incorporating a distribution over unobserved context into the partner model.
We focus on symbolic planning.
\justin{What would codegen output look like without symbolic planning?
Codegen gives you a structured output that you would have to linearize. Difficulty in linearization was the whole point of using code. Impasse}

\section{Background}
This section provides the background on \textsc{OneCommon} \citep{onecommon},
the grounded collaborative dialogue task of focus.
\textsc{OneCommon} is a particularly difficult collaborative reference game,
played between an agent and partner.
The agent and partner's shared goal is to find one dot that is visible to both,
one dot in common.
This is complicated by the fact that the agent and partner have different but 
overlapping views, illustrated in Figure \ref{fig:oc},
leading to perspective-dependent ambiguity.
For example: A dark dot to the agent may not be dark to the partner, depending on what they see.
Games are constructed so that the agent and partner share between 4--6 dots,
with fewer shared dots resulting in increased ambiguity and therefore game difficulty.
We focus on the most challenging setting, where 4 dots are shared.

At each turn, the agent must read the partner's utterance
and understand it given the dialogue history, all previous utterances,
as well as the dialogue context, the visual representation of dots.
The agent must then either give a response or select a dot.
Once a dot is selected by either the agent or partner, the other player
must also select a dot and the game ends.
Both the agent and partner win if the selected dots match.
The partner is not notified of which dot the agent selected
until after the game ends, and vice versa.

\begin{comment}
\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{imgs/Onecommon System.png}
\caption{\label{fig:system}
Placeholder dialogue system figure.
}
\end{figure}
\end{comment}

\begin{figure*}[t]
\centering
\includegraphics[width=2\columnwidth]{imgs/Onecommon System horizontal.png}
\caption{\label{fig:system}
At each turn, the agent first reads the partner utterance, parsing it into a plan
and generating a code fragment.
The fragment is composed into a Python function, executed, 
and used to update the agent's belief state.
The agent then plans what to say, and generates a response given the plan.
}
\end{figure*}

\section{Read-plan-write}
\justin{How to separate general from domain specific?}
% High level goal
%This section provides a high-level overview of the inputs and outputs of our dialogue system, Read-plan-write (\system{}), with implementations given in Section \ref{sec:implementation}.
This section details the components of our dialogue system, Read-plan-write (\system{}).
Our goal is to build a grounded dialogue system that,
given dialogue context and history,
produces a response to the partner's last utterance
such as giving an answer if asked a question,
asking their own question, or ending the game.
%\daniel{and chooses a dot at the end?}
We propose a dialogue system, \system{},
which follows a classical dialogue system pipeline decomposition:
reading, planning, and writing \citep{young2006pomdp,young2013pomdpsurvey,he2018dnd}.
%\justin{do i need to say more about this, eg why we do this?}
%The modular pipeline allows us to build in inductive biases that allow the targeted use of a general-purpose large language model in combination with high-precision task-specific components.We provide an illustration of the system in Figure \ref{fig:system}.

\subsection{Symbolic representations}
At the heart of our system is a symbolic meaning representation, which we refer to as a plan.
We assume that plans hold the game-relevant information used to generate a particular utterance. Reading involves inverting that generative process, parsing utterances into plans, while writing involves applying that process, describing plans via utterances.
Symbolic plans allow the system to
directly and efficiently synthesize past information,
as well as plan future actions.

In our formulation of \textsc{OneCommon},
plans consist of
a dialogue act, a set of dots under discussion, and a confirmation of the previous turn.

Given the nature of \textsc{OneCommon}, where 
referring expressions are pivotal to understanding,
we restrict our focus to dialogue acts related to referring expressions.
We consider 3 coarse dialogue acts:
\textsc{start}, \textsc{follow-up}, and \textsc{select}.
\textsc{Start} utterances introduce a new set of dots under discussion.
For example, ``Do you see a black dot?''
\textsc{follow-up} utterances build on previously introduced sets, such as ``Yes, is it larger than all the others?''
Note that this is different from the set of mentions in the utterance, as the set under discussion is just the black dot, and not ``all the others''.
\textsc{follow-up} utterances can be questions can discuss previously unmentioned properties of already mentioned dots or add new dots to the set under discussion.
\textsc{select} utterances communicate an intent to end the game, as well as which dot to select.
Finally, utterances that do not correspond to any of these dialogue acts are labelled \textsc{None}.

Confirmations take one of three values: \textsc{yes}, \textsc{no}, or \textsc{None}, and serve as answers to questions from the previous turn.
We make the simplifying assumption that the system must only deal with binary questions, and ignore others.

\textbf{Belief state}


\subsection{Read}
The goal of reading is to parse partner utterances into plans and keep track of the common ground built over the course of the dialogue.


The Python function is a translation of the set of constraints expressed in the partner utterance.
Following \citet{codeaspolicies2022} and \citet{vipergpt},
we assume access to an existing library of functions.
In \textsc{OneCommon}, we rely on a library of visual primitive functions
that can be used to evaluate properties of dots:
the size, color, and relative position of a single or group of dots.
We provide an example below, where the partner offers the first utterance
in a game:

\begin{Dialogue}
    \Partner{Do you see a black dot?}
    \AgentDo{%
      def turn(state):\newline
      \strut~~state = set()\newline
      \strut~~for dot in single\_dots():\newline
      \strut~~~~if is\_dark(dot):\newline
      \strut~~~~~~state.add(dot)\newline
      \strut~~return state
    }
\end{Dialogue}%

\noindent This is a \textsc{start} utterance, as there are no
previous utterances to \textsc{follow-up} on.
The generated Python function, \texttt{turn(state)},
takes a set of previously discussed dot configuration possibilities \texttt{state}
and filters out unmentioned sets of dots.
In this example, 
the Python function loops over all individual dots returned by the function
\texttt{single\_dots()} and returns the set of all dark dots.
As this is a \textsc{start} utterance, the argument \texttt{state} is not used.

Each turn calls a set of library functions in order to
express the logical constraints of the utterance.
The function \texttt{is\_dark(dot)} returns true if the greyscale dot is above a certain color threshold.
Similar to Code as Policies \citep{codeaspolicies2022},
we define a library of Python functions by hand.
Since the generated code is in Python, the model is able to flexibly
combine our custom library with other Python code such as 
list comprehensions and standard library functions.
The function \texttt{is\_dark} is an example of such a library function,
which is used to ground reasoning in the visual context.
In general, there may be any number of library functions calls in the function body.
The full library of functions is given in Appendix \ref{sec:library}.

Naturally, as this example was the first turn, there is no partner response to any previous turns.
Responses either confirm or deny a previous dot mention,
and can take three values: \textsc{Yes}, \textsc{No}, or \textsc{None}.

Next, we provide an example of a \textsc{follow-up} utterance:

\begin{Dialogue}
    \Partner{Do you see a big black dot?}
    \AgentSay{Yes, next to a small grey dot.}
    \Partner{Yes, is there also a light dot to the left?}
    \AgentDo{%
      def turn(state):\newline
      \strut~~for a,b in state:\newline
      \strut~~~~for dot in single\_dots():\newline
      \strut~~~~~~if (\newline
      \strut~~~~~~~~is\_light(dot)\newline
      \strut~~~~~~~~and is\_left(dot, [a,b])\newline
      \strut~~~~~~):\newline
      \strut~~~~~~~~state.add([a,b,dot])\newline
      \strut~~return state
    }
\end{Dialogue}%

\noindent This utterance confirms the plan of the previous turn 
and also builds upon that plan, adding a new dot.
For \textsc{follow-up} utterances,
the argument \texttt{state} is the output of a previous turn.
and contains all pairs of dots that are black and grey, as well as the correct sizes.
The \texttt{turn} function will return all dot triplets that satisfy the follow-up question.
\textsc{follow-up} utterances always refer to a previous turn.
Similar to \citet{sm}, we utilize task-specific heuristics for resolving the \texttt{state} variable,
which we describe in Section \ref{sec:read}.
Subsequent partner utterances are likewise parsed into Python functions,
using the parsed previous utterances as context.
\justin{Need to give example of passing \textsc{state} variable and state list.
Probably pseudocode.}

%State tracking
In addition to tracking the output of every previous turn's plan,
we also maintain a probabilistic belief state over task-specific quantities.
This belief state is updated via Bayes' rule using a model of partner responses given
the agent's previous plan and belief state \citep{young2006pomdp}.
In \textsc{OneCommon}, the belief state is a distribution over all possible shared dot configurations.
The response model models the partner response given which dots are shared (tracked by the belief state) and 
agent's previous plan.
We build on the response model from \citep{ocp}, detailed in Section \ref{sec:implementation}.
% would be cool if the model handled conditional yes as well. eg no explicit yes, but asked a follow-up.
% would language models know how to handle that?

Lastly, we give an example of a \textsc{select} utterance,
which communicates an intent to end the game.

\begin{Dialogue}
    \Partner{Do you see a big black dot?}
    \AgentSay{Yes, next to a small grey dot.}
    \Partner{Let's select the small one.}
    \AgentDo{%
      def turn(state):\newline
      \strut~~for a,b in state:\newline
      \strut~~~~~~if is\_small(dot)): state.add([b])\newline
      \strut~~return state
    }
\end{Dialogue}%

\noindent Similar to the other dialogue acts,
\textsc{select} utterances are parsed into a Python function
which itself calls library functions.
The \textsc{turn} function filters the state down to a list of single dots,
which are the candidates for selection.

\subsection{Plan}
The belief state is used to construct the agent's next plan.
The agent chooses a plan based on information gain \citep{lindley}.
Ideally, the agent would optimize an objective based on both information gain and communicative cost or surprisal.
Rather than directly optimize such an objective,
we instead introduce task-specific constraints on plans and optimize information gain under those constraints.
We provide the details in Section \ref{sec:implementation}.

\justin{In-depth questions are key to building common ground.
In-depth questions without coreference are impossible to understand.
Coreference without grounding is always incorrect.
Symbolic planning on top of dialogue state tracking makes this possible.}

%Generation
\subsection{Write}
Given a plan, responses are generated with rule-based templates.
Writing uses one simple template for each of the three dialogue acts,
illustrated in Figure \ref{fig:system}.
The system capitalizes on a special characteristic of cooperative two-player games: 
we can control the trajectory of the conversation,
allowing the system to only support a small subset of all possible dialogues.
In practice, this could be extended to more flexible albeit higher latency response generation.

%Just describe what the method is.
%Criticism 1: Only one dataset, how is the method generally applicable? May not need to mention this in the paper, but will come up in the rebuttal. Be upfront in limitation.
%Criticism 2: Vision+language baseline.

\section{System details}
\label{sec:implementation}
This section provides the parameterization and implementation details of the system.

\subsection{Read}
\label{sec:read}
%1) confirmation
%2) Dialogue act classification.
%3) Reference resolution and follow-up chains (slot carry-over). ;(Tree extension would be cool)

Intro sentence

\textbf{Plan parsing}
We prompt a large language model to obtain plans from utterances.
For dialogue act and response classification, as well as
cod generation, we use few-shot prompting.
The examples in all prompts are constructed based loosely on
5 dialogues from static \textsc{OneCommon} data collected by \citet{onecommon}.

For code generation, we craft 2 synthetic full multi-turn dialogue examples,
with the goal of covering as many library functions as possible.

\textbf{Library}
Describe library design

\textbf{Belief state and response model}
The agent maintains a belief distribution over which of the 7
dots are shared, represented as a distribution over boolean vectors of size 7.
Maintaining and updating this belief distribution requires both a prior over shared dots
and a response model.

We utilize a globally normalized prior over boolean vectors.
The prior's energy function is derived from the minimum spanning tree of the shared dots, where the weights of edges are determined by the ranking of how close dots are and the score of a tree is the sum of the weights of edges.
We provide details in Appendix \ref{sec:prior}.
We use the response model from \citep{ocp}, which explicitly accounts for perspective dependent ambiguity
by including a confusion model that accounts for the partner mis-resolving dots due to differences in perspective.

Given a prior, the agents updates its belief via Bayes' rule if the last plan is either confirmed or denied,
and if the agent can resolve the partner's dot mentions.
This means 

\subsection{Plan}
\label{sec:plan}
Planning chooses the most informative dialogue act and set of dots to ask about, under a set of constraints.

We choose between dialogue acts \textsc{start}, \textsc{follow-up}, and \textsc{select} based on informativity and the probability of successful coordination. If a dot is shared with probability above a particular threshold, the agent chooses to \textsc{select}.
Otherwise, the agent chooses to \textsc{start} asking about 
new dots or a \textsc{follow-up} question
about an existing set based on information gain \citep{lindley}.

We heuristically encode communication cost by constraining \textsc{start} questions to only ask about two dots, and \textsc{follow-up} questions
to ask about a single new dot.

\section{Experimental setup}
\label{sec:exp}

We first conduct a preliminary study on reference resolution.
We use simple template generations for partner utterances, asking if the agent sees a plan consisting of two dots,
and evaluate whether different systems are able to accurately understand the generations.
For this evaluation alone, the resolution system has the same view as the partner.
We report the exact match accuracy of the resolved plan.

Next, our primary form of evaluation is
human evaluation in \textsc{OneCommon} dataset \citep{onecommon},
specifically game success with human partners.
We also report the number of turns taken.
We do not emphasize game length, as game length is at odds with overall success
for conservative agents.
\justin{num trials, payment}

We also report success in selfplay, where agents play with a copy of themselves.
Selfplay decreases the noise in evaluation due to linguistic and partner skill diversity.

%Analysis
%Claim 1. Planning
%Claim 2. Code
%  Ablations on shorter prompts in terms of accuracy and speed. What works?

\textbf{Systems}
For reference resolution, we compare
\system{} versus a code-generation system with visual components ViperGPT \citep{vipergpt},
a visual question-answering system BLIP2 \citep{},
and the previous state of the art for \textsc{OneCommon} RNN \citep{fried}.

For human evaluation, we compare \system{}
versus the previous state of the art RNN \citep{fried} as well as
human agents, with a human partner.

For selfplay, we consider a wider variety of systems than human evaluation, due the reduced cost of evaluation.
We compare \system{} to RNN \citep{fried},
a prompt-based text-only language model baseline,
ViperGPT \citep{vipergpt},
and BLIP2 \citep{}.
We also run smaller-scale ablations of the planning

\textbf{Hyperparameters}
For the Partner Planner, we set the response faithfulness probability $\theta = 0.95$.
We determine the selection entropy threshold by running grid search over $\tau \in \set{1,1.5,2}$ on symbolic selfplay, and pick the value with the highest success rate.

The partner response classifier is a RoBERTa model \citep{roberta}, with a second stage of fine-tuning performed on 147 annotated dialogue turns.
We annotate the text each turn as a confirmation, denial, or no response.
The model is originally fine-tuned on sentiment \citep{heitmann2020}.

For mention classification from text, we use a mention prediction network from past work \citep{fried}.
The mention prediction network explicitly models relationships between mentions using a conditional random field with neural potentials.

\section{Results}

\begin{table}[!t]
\centering
\begin{tabular}{lr}
\toprule
Agent                    & Acc\\
\midrule
\system{}                &  23/25 \\
GPT4-MC                  & \%  \\
\citet{fried}            & 4/25  \\
BLIP2                    & \%  \\
\bottomrule
\end{tabular}
\caption{\label{tbl:refres}
Reference resolution
}
\end{table}

\textbf{Reference resolution}
We find that vision-based models as well as 
language-only models are much less accurate than
a code-based grounding approach for this task,
shown in Table \ref{tbl:refres}.
We hypothesize that this is due to out-of-domain images
and reasoning difficulty respectively.
We also find that the previous state-of-the-art approach,
RNN \citep{fried}, has difficulty recognizing simple utterances.
We hypothesize that this is also caused by distribution shift:
the reference resolution model of RNN captures both
language descriptions as well as the distribution of dots
preferred by the human demonstrations it was trained on,
resulting in failure to generalize to simple 2-dot combinations.
The code-generation approach does not have this issue,
as it only captures information in language descriptions
rather than the marginal dot configuration reference distribution.

\textbf{Human evaluation}
Speculative: \system{} outperforms the original human performance in
the \textsc{OneCommon} dataset,
where humans were coached in order to improve data quality \citep{onecommon}
(also cite personal communication with Udagawa).
Speculative: \system{} takes more turns on average, but
has a higher success rate.
We hypothesize that this is caused by shorter human partner
responses to the system.
The average number of words per partner utterance when playing with
\system{} is XX, as opposed to YY with another human.

\begin{table}[!t]
\centering
\begin{tabular}{lrr}
\toprule
Agent                   & Success & Avg \# turns\\
\midrule
\system{}               & 75.0\%        & 7.\\
GPT4                    & \%  & \\
Human                   & 65.8\%  & 4.\\
\citet{fried}           & 62.4\%  & \\
\bottomrule
\end{tabular}
\caption{\label{tbl:human-eval}
The success rate of different agents with human partners on the hardest setting of \textsc{OneCommon}, with 4 shared dots.
A higher success rate is better.
The human performance is from the \textsc{OneCommon} dataset
\citep{onecommon}.
}
\end{table}

\textbf{Selfplay}
A similar pattern holds in selfplay,
where \system{} outperforms all other systems and humans.
Analysis TBD.

The average number of turns per game of \system{} is more comparable 
to the average for human-human games, likely due to \system{}
being constrained to ask a new question at every turn.

\begin{table}[!t]
\centering
\begin{tabular}{lrr}
\toprule
Agent                   & Success & Avg \# turns\\
\midrule
\system{}               & 78.43\% & 4.94\\
GPT4                    & \%      & \\
Human                   & 65.8\%  & 4.97\\
\citet{fried}           & 62.4\%  & \\
BLIP 2                  &         & \\
ViperGPT                &         & \\
\bottomrule
\end{tabular}
\caption{\label{tbl:selfplay}
The success rate of different agents in selfplay on the hardest setting of \textsc{OneCommon}, with 4 shared dots.
A higher success rate is better.
The human performance is from the \textsc{OneCommon} dataset
\citep{onecommon}.
}
\end{table}

\justin{complete garbage below here, haven't finished un copy-pasting.}

\section{Analysis}
We present additional analysis experiments in this section.

\justin{need to compare our system to the 
second best thing.}

\textbf{Static reference resolution}
worry: if make claim that bad at refres, might be hard for reviewers to understand. How strong is the code generation system at understanding normal text?
Static reference resolution results, comparison to Fried baseline.
Conclusion: Not strong, but due to the 2-player nature of the game,
able to steer the conversation.
Makes intentional planning even more important (avoid weaknesses / ambiguity).


\textbf{Prompt style analysis}
Length of prompts and number of roundtrip errors.

\begin{table}[!t]
\centering
\begin{tabular}{lrrr}
\toprule
Prompt style                   & Acc & Time & Len\\
\midrule
\system{}                      & \%  &    &  \\
Single prompt                  & \%  &    & \\
Shorter                        & \%  &    &  \\
\bottomrule
\end{tabular}
\caption{\label{tbl:prompt}
An analysis of different code generation prompt styles.
}
\end{table}

\textbf{Use a different system}
s/gpt4/starcoder?
s/gpt4/gpt3.5?

\textbf{Follow-up vs just describe everything}
See if follow-up heuristic vs info gain surprises user.

\textbf{Static plan evaluation}
Directly modeling and marginalizing over unobserved partner perspective results in fewer
errors than the egocentric heuristic from \citet{fried}, obtaining a 12\% reduction in errors as shown in Table \ref{tbl:static-plans}.
Additionally, the number of incorrectly resolved plans from Planner is much lower than an ablated planner that does not model unshared dots, a 72\% reduction.

We hypothesize that the baseline model of \citet{fried} makes fewer errors because it is able to repeat plans mentioned in the static dialogue.
The Partner Planner does not often repeat plans, as there is little information gained.
When restricted to the plan proposals from the first turn of a static human-demonstrated dialogue, where no plans can be copied, the Partner Planner outperforms the baseline by a larger margin, as shown in Table \ref{tbl:static-plans}.

We note that the absolute number of resolution errors is small relative to the total number of turns.
We hypothesize that this is because of the nature of static evaluation.
Static evaluation considers next step plan proposals given human dialogue,
preventing agents from steering the dialogue themselves.

\begin{table}[!t]
\centering
\begin{tabular}{lrr}
\toprule
Agent                   & Success & Avg \# turns\\
\midrule
\system{}               & \%        & \\
GPT4                    & \%  & \\
Human                   & \%  & \\
\citet{fried}           & 62.4\%  & \\
Human                   & 65.8\%  & 4.97\\
\bottomrule
\end{tabular}
\caption{\label{tbl:selfplay}
The success rate of different agents in selfplay on the hardest setting of \textsc{OneCommon}, with 4 shared dots.
A higher success rate is better.
The human performance is from the \textsc{OneCommon} dataset
\citep{onecommon}.
}
\end{table}

\textbf{Symbolic selfplay}
The Partner Planner achieves strong performance in symbolic selfplay, as shown in Table \ref{tbl:selfplay}.
The ablated version, which does not model unshared dots, also performs well, but worse than the full Partner Planner.
This demonstrates the utility of modeling unshared perspective.

Both the full and ablated Partner Planner outperform the baseline of \citet{fried} and coached human performance from the training data of \citet{onecommon}, demonstrating the utility of partner modeling.
Much of this success comes from the ability to control the
belief entropy selection heuristic, as shown by the performance of the ablated Partner Planner over human performance.
The selection heuristic encourages the Partner Planner to be more patient and gather more information before selecting than most human participants,
reflected in the average number of turns per game.

\section{Future Work}
%\textbf{Incorporate language, discuss fidelity and OOD-ness for generation/resolution}
%\textbf{Extension to \textsc{MutualFriends}}
For future work, the current model will be extended with text generation in order to play \textsc{OneCommon} with humans.
Preliminary experiments using utterance generation methods from prior work by \citet{fried} found that the plans proposed by the Partner Planner were out-of-distribution for supervised methods, resulting in many errors in generation.
Further experiments with a templated generation system found that the resulting templates were too difficult to understand for human partners.
Future work will experiment with generation systems that account for utterance processing cost, balancing the amount of information conveyed and fluency.

Additionally, the scalability of the method will be tested on dialogue games with larger environments.

%\section{Conclusion}
%TBD

\section*{Acknowledgements}
Ge Gao, Omer Gul, Woojeong Kim, Celine Lee, Jack Morris, Chenran Ning, Alane Suhr, Nicholas Tomlin, Jiawei Zhou.

\section*{Limitations}
There is a tradeoff between expressivity and  controllability for dialogue partner modeling.
In this work, we emphasize controllability by explicitly modeling certain aspects of the partner perspective while not accounting for others, resulting in biased agents.
The failure to account for
particular partner aspects may affect fairness and equity with a diverse pool of partners.

%\section*{Ethics Statement}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

%\section{Templated generation}
%\label{sec:generation}
%TBD


\section{Partner Modeling in Reference Games}
\label{sec:planning}

Collaborative reference games pair an agent and a partner in order to agree on a shared object through natural language dialogue.
At each turn, the agent or partner may decide to terminate the game and make a selection.
Once either the agent or their partner terminates, the other player must also act (without observing the other's choice). If both agent and partner agree, both win; otherwise, both fail. 

Our approach to reference games separates planning of utterances (choosing what to talk about) from surface realization (choosing how to say it).
At each turn, our agent produces an utterance plan $x$ by using a \emph{partner model}, which simulates the partner's possible responses, $y$, given their hidden perspective, $z$.
The agent uses the partner model to infer the partner's perspective and predict the partner's responses to the agent's plans.
We first give an overview of the partner model 
and planning procedure in this section.

% restate goal and contribution
\noindent \textbf{Partner model}
We model the partner's perspective as a latent variable, infer the value for this variable over the course of a game, and use it to plan generation.
This contrasts with a typical egocentric heuristic, as used in \citet{fried}, which assumes the partner's perspective
is identical to the agent's.

The partner model predicts a distribution over the partner response $y$ given the agent plan $x$ under the latent shared perspective $z$, and decomposes as:
$$p(y \mid x) = \sum_z p(y \mid x,z)p(z).$$


% planning
\noindent  \textbf{Planning}
The agent uses the partner model to plan
what to say next, by choosing
% Planning finds 
the plan $x$ that maximizes the expected information gain  \citep{lindley} about the shared perspective $z$,
defined as
$$\argmax_x H[z] - \Es{y \mid x}{H[z \mid x,y]},$$
where $H[z]$ is the entropy of the prior\footnote{The prior entropy $H[z]$ in the definition of information gain is constant with respect to
the plan $x$, and can be dropped from the objective.}
 and $H[z\mid x,y]$ the posterior,
which requires marginalizing over $z$.

% \noindent  \textbf{Generation}
%Given a plan $x$, we generate a descriptive utterance that conveys this plan faithfully to the partner using a templated generation system. We provide the details in Appendix \ref{sec:generation}.

% belief update
\noindent \textbf{Belief update}
After observing the partner response $y$, the agent updates its belief $p(z)$ over the shared
with Bayes' rule:
$$p(z \mid x, y) = p(y \mid x,z)p(z) / \sum_z p(y,z \mid x)$$
This is performed iteratively after each turn,
and requires marginalizing over possible shared perspectives.

\noindent \textbf{Selection}
After gathering information through planning and
incorporating information through belief updates,
the agent must decide when it has built enough common ground, collaboratively identifying a shared dot with its partner.
We set a threshold on the belief entropy, $H[z]$,
which determines when the agent should transition
from information gathering to ending the game.
%\daniel{How is selection performed?}
% game specific, will loop back to this later

\section{Planning in \textsc{OneCommon}}
\label{sec:plan-oc}

We focus on applying partner modeling to \textsc{OneCommon} \citep{onecommon}, which represents a class of collaborative reference games \citep{mf,pb} where only a subset of each player's perspective is shared, resulting in perspective-dependent ambiguity. 
% \citep{onecommon}.

In \textsc{OneCommon}, the agent's known perspective $\mcD$ consists
of 7 dots in its view.
%The agent perspective stays constant for a particular game.
Each dot has a set of features: size, color, and position in the 2D plane.
All features are continuous.
The main challenge of the game is that the partner perspective is also a view of 7 dots,
Between 4--6 of those dots are shared with the agent
perspective which we denote as the shared perspective $z$.
Additionally there are a set of unshared dots $u$ the fill out the partner perspective.
An example is given in Figure \ref{fig:xzy}.
Note that a smaller number of shared dots increases the likelihood that plans get misresolved to unshared dots, increasing perspective-dependent ambiguity.

The agent communicates with the partner by producing an utterance plan, $x$, which it then describes in natural language.
This plan is a subset of the dots in the agent view, $x \subseteq\mcD$,
that the agent will ask the partner about. The partner gives a response $y$ to the plan $x$, given their perspective $z$.
In \textsc{OneCommon}, the partner responds in natural language;
however, the partner model only models the partner response as a confirmation $y\in\set{\textsc{yes}, \textsc{no}}$,
obtained by classifying natural language responses.


Exact planning is intractable because the objects in the partner perspective have continuous-valued features.
In this section, we describe simplifying assumptions for the partner model and inference procedure that make planning tractable.

\begin{figure}
\centering

\scalebox{1.1}{
\begin{tikzpicture}

%\filldraw[gray!40] (0,0) circle (.3em);
%\filldraw[gray!100] (.25,0) circle (.38em);
%\filldraw[gray!160] (.5,0) circle (.45em);

\filldraw[gray!160] (-.2,.6) circle (.45em); % 76
\filldraw[gray!40] (-.7,-.3) circle (.4em); % 51
\filldraw[gray!40] (.4,.1) circle (.3em); % 52
\filldraw[gray!100] (.8,-.1) circle (.3em); % 66

% left
\filldraw[gray!140] (-1.3,-1.2) circle (.3em); % 13
\filldraw[gray!100] (-1.6,-1.8) circle (.3em); % 14
\filldraw[gray!110] (-.2,-1.1) circle (.4em); % 74

% right
\filldraw[gray!100] (-1,1.5) circle (.3em); % 28
\filldraw[gray!100] (-.5,1.6) circle (.38em); % 69
\filldraw[gray!160] (.8,1.8) circle (.45em); % 26

% agent plans 76
% \node[
%     draw,circle,dashed,blue,thick, minimum size=1.2em,
%     %label= left:$x$
% ] (x) at (-.2,.6) {};
% % m label
% % \filldraw[gray!40] (.4,.5) circle (.3em); % 52
% \node[
%     draw,circle,dashed,red,thick, minimum size=.6em,
%     %label=left:$m$
% ] (m) at (.4,.1) {};

\draw[blue,thick] (-.3,-.7) circle (5em);
\draw[red,thick] (.2,1) circle (5em);

% s label
\draw[draw=black] (-1.5,-1.6) rectangle (0.1,0);
%\node (s) at (-.8,1) {$s$};

% u label
% \draw[draw=black] (-1.2,1.3) rectangle (1.1,2.1);
%\node (u) at (-1.05,1.95) {$u$};
\node (u) at (-1.4, 1.7) {$u$};
\node (s) at (-1.4,0.5) {$z$};
% z label
%\node (z) at (-2.7,1) {$z$};
% d label
\node (d) at (-2.7,-.7) {$\mcD$};
\node (x) at (-1.7,-0.8) {$x$};
 
% \node (xe) at (-3.23,-3) {$x=$};
%\filldraw[gray!40] (-2.5,-3) circle (.3em); % 52
% \filldraw[gray!160] (-2.5,-3) circle (.45em); % 76
% \draw[dashed,blue,thick] (-2.5,-3) circle (.6em);

\node (v) at (-0, -3) {\textit{Do you have a triangle of one  gray dot ... }};

\node (y) at (-0,-3.6) {$y=$ \textsc{No}};
% \node (m) at (0,-3.6) {$m=$};
% \filldraw[gray!40] (.7,-3.6) circle (.3em); % 52
% \draw[dashed,red,thick] (.7,-3.6) circle (.5em);

%\node (v) at (-0, -4.2) {$y=$ No, I don't see those.};

\end{tikzpicture}
}
\caption{In \textsc{OneCommon},
the agent's perspective $\mcD$ is represented by the large blue circle,
and the partner's unobserved perspective by the red.
The shared dots $z$ are in both perspectives,
while the unshared dots $u$ are only in the red circle. 
The agent plan $x$ is given by the dots in the box,
and also described in language.
The partner response $y$ is a binary confirmation.
}
\label{fig:xzy}
\end{figure}

\subsection{Partner model}

% There are two challenges for planning with a partner model in \textsc{OneCommon}.
% First, marginalizing over the unobserved partner perspective, $\int_z p(y,z\mid x)dz$, is intractable because the dots in $z$ have continuous-valued sizes, colors, and positions.
% % This is solved by discretization.
% We address this using feature discretization.
% Second, the number of possible discretized partner perspectives $z$ is still large.
% We make marginalization tractable by decomposing the partner perspective $z$ into wo components: dots which are shared and unshared with the agent, %dots which are shared and unshared with the agent,
% and constrain the partner model to reason about them separately.
% breaking down the computation of marginalization into tractable steps.

We build a partner model by factoring the shared perspective $z$ and partner response $y$ as illustrated in Figure \ref{fig:xzy}. 
Formally,
\begin{align*}
p(y \mid x) &= \sum_z p(y\mid x,z)p(z) \\
&= \sum_{z,u}  p(y \mid x, z, u)p(z)p(u),
\end{align*}
where we introduce the latent variable $u$ representing the unshared dots in the partner perspective.

The shared dot prior, $p(z)$, is a distribution over subsets of $\mcD$, indicating which dots in the agent perspective $\mcD$ are shared with the partner.
The model $p(z)$ is initially uniform over dot subsets at the start of a game,
but is updated given evidence from the partner response $y$ at the end of each turn, $p(z \mid x, y)$. For notational simplicity we focus on the first turn.

The unshared dot prior, $p(u)$, is a distribution over the remaining partner dots.
Since the dots in $u$ are unobserved by the agent, we parameterize $p(u \mid s)$ using a uniform distribution over discretized features for each dot.
We ignore spatial features for dots in $u$
and discretize the other originally continuous features: size and color.%
\footnote{
We discretize size and color uniformly into 3 buckets based on their absolute range
across \textsc{OneCommon}.
}


%\justin{optional ->}
%The support of $p(u \mid s)$ is larger than $p(s)$: for $|u| = 3$, there are $9^3 = 728$ feature combinations.

%\justin{optional ->}
% The size of the support of $p(z)$ increases multiplicatively with the size of $p(u \mid s)$, making marginalization computationally costly.
% \daniel{insert forward reference here to where you describe how you address this?}

%\noindent \textbf{Response}
% The response model,
% % $$p(y \mid x,z) = p(c \mid x,z)p(m\mid z)p(w\mid c,m),$$
% factors into unshared, confirmation, and word models.
The confirmation model, $p(y \mid x,z)$, checks whether a partner will confirm or deny the agent plan. The partner confirms if they are able to resolve the plan $x$ to their perspective. Given a fully observed $z$ and $u$, resolution of a plan $x$ is performed by matching the features of $x$ to $z$ and $u$.
There are no trained parameters in resolution, as it depends only on the features of dots in $x$, $z$, and $u$.
See Appendix \ref{sec:feature-resolution} for the details of feature-based resolution.

In order to avoid jointly enumerating $z$ and $u$, the model reasons separately about $z$ and $u$ by making the simplifying assumption that plans are fully in $z$ or $u$.
This means that the model will deny if part of $x$ is in $z$, while the remainder is in $u$ (and $x$ is not fully contained in either $z$ or $u$):
\begin{align*}
p(y=\textsc{no}\mid x) &=\sum_{z,u} p(y=\textsc{no} \mid x, z, u)p(z,u)\\
&=\sum_z p(y=\textsc{no}\mid x, z )p(z)\\
&\quad \cdot \sum_u p(y=\textsc{no}\mid  x, u )p(u).
\end{align*}
Given the unsuccessful resolution of $x$ to both $z$ and $u$, the partner denies accurately with probability $\theta$, a hyperparameter.

% The confirmation model, $p(c \mid x,z,u)$, 
% checks if the partner confirms our plan
% based on their perspective.
% We make the simplifying assumption that a confirmation,
% $c = \text{\textsc{yes}}$, indicates that a plan $x$ resolves fully to either \justin{does either imply exclusive? its not an exclusive or} $z$ or $u$.\footnote{We ignore overlaps for computational efficiency, see Appendix \ref{sec:marginal-confirmation}}.
% % This means that the partner disconfirms if part of $x$ resolves to $s$ and the remainder to $u$,
% % and there is no other way to resolve $x$ to either $s$ or $u$.
% % We demonstrate how this assumption improves the computational complexity of computing the marginal confirmation distribution
% % in Appendix \ref{sec:marginal-confirmation}.
% When considering matches to $z$ all bucketed features (size, color, and spatial position) are checked.\footnote{
% The pairwise spatial position between dots  is bucketed into four
% relations: 
% above-left, above-right, below-left, or below-right.
% }
% When considering possible matches to $u$, spatial features are ignored for efficiency.

%\noindent \textbf{Response}
%Given a successful resolution of $x$ the partner confirms with probability $\theta$.
% The same parameter $\theta$ also determines disconfirmation given unsuccessful resolution.
% \daniel{how is $\theta$ set?}


% The mention model, $p(m \mid z)$, is a distribution over
% possible partner referents, represented as subsets of $z$.
% We assume that partner referents are not split between $s$ and $u$:
% mentions $m$ are chosen by first choosing either $s$ or $u$ with equal probability, then uniformly choosing a subset of dots from either $s$ or $u$ respectively.
% % allows same distributive trick

%The utterance model, $p(y \mid c)$, is a uniform distribution over sentences that confirm or deny the plan. \srush{This part needs to actually be written.}

\subsection{Inference}
\label{sec:inference}

% In the remainder of this section, we give the specifics of inference in \textsc{OneCommon}.
% As the goal of \textsc{OneCommon} is to find a dot in common,
% The agent plans by optimizing 
% Throughout, the agent marginalizes over the uniform $p(u\mid s)$,
% a conservative assumption that all unshared dots configurations are possible.
% This causes the agent to avoid plans that are likely to be incorrectly resolved to the unshared dots regardless of any information gathered,
% resulting in richer, more specific plans.
%\daniel{Justify this better, e.g. move the text from  "Belief update" (``avoid plans... any information gathered'') here. Or, might be worth adding something to reinforce that beliefs do still update in a useful way if possible, as otherwise this may seem like a big limitation. This doesn't have to come at this point in the paper, but it could help generally to have e.g. a qualitative example showing how the belief over the observed dots changes as we get more utterances.}

During inference, we need to compute $p(y \mid x)$ for all plans $x$,
which can be done in two steps: First, we marginalize over the unshared dots $u$.
This marginalization can be expressed in closed form.
For the details, see Appendix \ref{sec:unshared-marginal}.
Second, we marginalize over the possible set of shared dots $z$.
The computational cost of marginalization is the size of the power set of $\mcD$, $O(2^{|\mcD|})$.

%\footnote{Marginalization over the unshared perspective in the partner model can be precomputed for an entire dialogue given the agent perspective $\mcD$, thanks to the specifics of the partner model parameterization. See Appendix \ref{sec:information-gain} for the details on precomputing marginal distributions for planning.}

We utilize this distribution to compute the posterior on the shared perspective $z$, after observing a partner response to a plan,
$$ p(z \mid x,y) = \frac{ p(z, y \mid x)}{p(y \mid x)}.$$
This posterior then allows us to perform optimization over plans with respect to the expected information gain, as well as update our beliefs given the partner response.

% Accounting for the unshared dots in the partner perspective in this manner encourages the agent to choose plans which are less likely to have counterparts that only the partner can see.

\noindent \textbf{Planning} Planning optimizes the expected information gain with respect to the shared perspective $z$:
$$\argmin_x \Es{y\mid x}{H(z \mid x,y)}.$$
Computing $p(y \mid x)$ has cost $O(2^{|\mcD|})$,
while there are also $O(2^{|\mcD|})$ plans.\footnote{
Plans $x$ are subsets of $\mcD$ that the agent would like to ask the partner about.
}
As a result, optimizing this objective takes $O(2^{2|\mcD|})$ computation, and is performed in less than one second on CPU.% in real-time.
% \daniel{might be a good place to say that this is still fast enough to run on a CPU in real-time, or give a forward ref to where you say that}

\textbf{Belief update}
The belief update directly uses the posterior distribution $p(z \mid x,y)$, as described in Section \ref{sec:planning}.

During gameplay in \textsc{OneCommon}, the agent either directly observes the symbolic response $y$ or receives a description of $y$ in natural language.
In order to process the natural language dialogue,
we use a classifier to extract $y$ from natural language. 
Additionally, the partner can mention dots of their own, either symbolically or described in text.
The agent incorporates partner mentions into its belief by treating them as a confirmed plan.
We use another classifier to extract partner mentions from text.
We give the details of both the response and mention classifiers in Section \ref{sec:exp}.

\textbf{Selection}
To determine when to select a dot, the agent uses a threshold on the entropy $H[z]$, given by the hyperparameter $\tau$.
The agent them communicates which dot to select by
describing the configuration of four dots with the
highest marginal probability of being shared,
as well as the dot within that configuration that is most likely to be shared.
The agent then selects the described dot.


\section{Feature-based resolution}
\label{sec:feature-resolution}
Feature-based resolution featurizes a plan $x$
then compares the features to all subsets of dots
in the partner domain $\mcD$.
The set of features used for each plan $x$ is given by the shape and size
each dot in the plan, bucketed into 3 bins based on the range of each feature.
The pairwise positions, limited to above-left, above-right, below-left, and below-right, are also contained in the feature set.
We provide an example of feature-based resolution in Figure
\ref{fig:resolution-example}.

Given a plan $x$, feature-based resolution
must compare all the features of the plan,
of which there are $O(|x|^2)$,
to all partial permutations of subsets size $|x|$ taken from $\mcD$,
of which there are $O(|\mcD|^{|x|})$.
This can be precomputed at the start of a dialogue.

When resolving $x$ to unshared dots, we ignore spatial features.

\begin{figure}[t]
\setlength{\abovecaptionskip}{0pt}

\centering

\scalebox{1.1}{
\begin{tikzpicture}


\filldraw[gray!160] (-.5,.5) circle (.45em); % 76
\filldraw[gray!40] (-.7,-.3) circle (.4em); % 51
\filldraw[gray!100] (.8,0) circle (.3em); % 66
\filldraw[gray!100] (-1,1.5) circle (.3em); % 28
\filldraw[gray!100] (-.5,1.6) circle (.38em); % 69
\filldraw[gray!160] (.8,1.8) circle (.45em); % 26
\filldraw[gray!160] (.6,1) circle (.45em); % 26

%\draw[dashed,blue,thick] (-.3,-.7) circle (5em);
\draw[red,thick] (.2,1) circle (5em);

\node[
    draw,circle,dashed,red,thick, minimum size=1.2em,
    label=below right:$a$
] (a) at (-.5,.5) {};
\node[
    draw,circle,dashed,red,thick, minimum size=1.2em,
    label=below right:$c$
] (c) at (.8,1.8) {};
\node[
    draw,circle,dashed,red,thick, minimum size=1.2em,
    label=below right:$b$
] (b) at (.6,1) {};

\node (f) at (.2, -1.5) {Feature representation};

\node[anchor=west] (f1) at (-2.75, -2) {Dot 1: Large, dark};
\node[anchor=west] (f2) at (-2.75, -2.5) {Dot 2: Large, dark, below-left Dot 1};

\end{tikzpicture}
}
\vspace{1em}
\caption{
An example of feature-based resolution.
The above feature representation for a pair of dots
resolves to dot configurations $\set{(a,b), (a,c), (b,c)}$.
}
\label{fig:resolution-example}
\end{figure}

\section{Resolution to unshared dots}
\label{sec:unshared-marginal}
The partner model, with the assumption that
$x$ cannot be split between $z$ and $u$,
is given by
\begin{align*}
&p(y=\textsc{no} \mid x)\\
&= \sum_{z,u}p(y = \textsc{no} \mid x,z,u)p(z)p(u)\\
&\approx \sum_{z,u}p(y=\textsc{no} \mid x,z)p(z) p(y=\textsc{no} \mid  x,u)p(u)\\
&= \sum_z p(y=\textsc{no} \mid x,z)p(z) \sum_u p(y=\textsc{no} \mid  x,u)p(u)\\
&= \sum_z p(y=\textsc{no} \mid x,z)p(z) \sum_u p(y=\textsc{no},u \mid  x).
\end{align*}
The probability a plan $x$ resolves to the unshared dots $u$ is
\begin{align*}
\sum_u p(y=\textsc{no},u \mid x)
&= 1-\theta {|u|\choose|x|}\frac{B^{2 \cdot (|u|-|x|)}}{B^{2\cdot|x|}},
\end{align*}
where $B$ is the feature bucket size,
given $|u| \ge |x|$.
This relies on the assumption that spatial features are ignored when resolving to unshared dots.

\section{Planning objective}
In addition to maximizing the information gain,
we also add an entropy rate constraint to the planning objective,
motivated by the entropy rate constancy and uniform information density hypotheses \citep{erc,uid,ercdial}.

In order to enforce a limit on the entropy rate, 
we limit the surprisal of the next mention proposals
under a mention model $p(x_t | x_1,\ldots,x_{t-1})$ \justin{how to include partner mentions and confirmations?}.
We could utilize the belief state $p(z = x_t)$ for this model \justin{include other history?}; however, this model is not sensitive to the ordering of plans in $h$.
We model mentions with an linearly decaying cache model,
\citep{cache}:
\begin{align*}
&p_{\text{cache}}(x_T \mid h)\\
&\propto \prod_d e^{\beta m([x_T]_d,h)
    \mathbf{1}([x_T]_d\in h)
}
e^{\alpha\mathbf{1}([x_T]_d \notin h)},
\end{align*}
where
$$m([x_T]_d,h) = T - \argmax_t t\mathbf1([x_T]_d \in h_t),$$
and $\beta$ and $\alpha$ are hyperparameters that control
the cost of unmentioned dots and recency bias respectively.

We also experiment with a supervised neural model \justin{include other history?}.

\section{Partner confusion model}
In the previous section, we introduced processing cost (as a channel capacity constraint) in the planning objective to limit the amount of information conveyed by an agent.
This is an anti-causal approach: Adding processing cost to the objective does not model the effects of a plan.
In this section we give a causal model of partner confusion as a result of processing cost.

The generative process for a response, given in Section \ref{sec:plan-oc}, is given by:
given a plan $x$, resolve that plan to the shared dots $z$ or unshared dots $u$. If the plan resolves to one or both, the partner gives a confirmation $y = \textsc{Yes}$.
Unfortunately, this partner model assumes superhuman levels of resolution.

In our human studies, we found that certain plans are difficult for humans to resolve. Plans that involve contextually vague color or size descriptors, or dots that are too far apart result in the denial of a plan that should have been confirmed.
Additionally, we believe some human players were frustrated by the information-dense descriptions of inhuman model plans, leading to poor success rates.
In this section we describe several changes to the partner model that better reflect the behaviour of a human partner.

There are three desiderata:
\begin{enumerate}
\item Spatial locality: The partner will deny a plan if the dots are too far apart
\item Channel capacity: The partner will deny a plan if there is too much new information
\item Information locality: The partner has limited memory and will treat repeated old information as new information
\end{enumerate}

We incorporate these desiderata by building them into the partner model.
We utilize a product of experts formulation,
where we combine the original partner resolution model
with a confusion model.
The goal of the confusion model is to model the effort a human partner would be willing to put into resolving a plan.
The product of experts formulation allows the confusion model to
vote against confirmation,
resulting in the response model predicting denial if the plan is theoretically resolvable, but too confusing for a human to actually understand.

Let $h$ be the history of all previous plans,
with $[h]_t = x_t$.
The full partner response model is given by
$$
p(y \mid x, z, u, h)
\propto \underbrace{p_r(y \mid x,z,u)}_{\text{resolution}}
\underbrace{p_c(y \mid x,z,h)}_{\text{confusion}}.
$$
The confusion model itself is a product of experts,
consisting of spatial and temporal confusion models:
$$
p_c(y\mid x,z,h) \propto
\underbrace{p(y \mid x,z)}_{\text{spatial}}
\underbrace{p(y \mid x,h)}_{\text{temporal}}.
$$

\subsection{Spatial model}
For the spatial model, we assume a plan is more likely to be denied if the dots in the plan are far apart.
A first approach would be to either use the distance between the furthest pair of dots, the sum of the pairwise distances between dots, or the area of the convex hull of the dots.
However, as the distances of dots may vary,
we instead use the relative rankings of dot distances.
This mimics one possible method of dot resolution, where the partner first finds an anchor dot then searches nearby to find the remaining dots in the plan.
As a result, we have
$$
p(y = \textsc{No} \mid x)
= \min_{d\in x} \frac{
\sum_{d'\in x} r(d,d')
}{
1+\sum_{i=7-|x|+1}^7 i
},
$$
where $r(d,d')$ gives the rank of dot $d'$ to $d$ in order of increasing distance given the agent's perspective.
%\justin{will fix up later}
nswer both time, and take that into account with the partner model.

\section{Supervised partner model}


\section{Selection objective}

\section{Describing plans}
The success of an agent that plans through a partner model is limited
by the accuracy of its partner model.
While the partner model predicts the partner's reaction to the agent's plan,
a human partner receives a natural language description of the agent's plan.
For the partner model to be accurate, the plan description must be precise.

Preliminary experiments indicate that a model from previous work \citep{fried}
loses precision when describing plans involving more than a single dot.
We hypothesize that this is due to data imbalance.



\end{document}

