% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
%\usepackage[review]{EMNLP2023}
\usepackage{EMNLP2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% New packages
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{mystyle}
\usepackage{multirow}
%\usepackage{fontawesome}
\usepackage{amssymb}
\usepackage{listings}

\usepackage{courier}

%\newcommand{\system}{ShrdLoc \faSearch}
\newcommand{\system}{SPC}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

\usepackage{tabu}
\newsavebox{\DialogueBox}

\newenvironment{Dialogue}[1][\small]{
    #1
    \def\arraystretch{1.3}
    \setlength\tabcolsep{7pt}
    \taburulecolor{lightgray}
    \vspace{.8em}
    \noindent
    \begin{tabu} to \columnwidth {|[2pt]lX}
}{
    \end{tabu}
    \vspace{.5em}
}

\newcommand{\Partner}[1]{Partner: & \UserUtt{#1} \\}
\newcommand{\AgentDo}[1]{
\\[-1.3em]
\multicolumn{2}{|[2pt]p{\linewidth}}{ 
\AgentAction{#1}
}
\\[.3em]
}
\newcommand{\AgentSay}[1]{Agent: & \AgentUtt{#1} \\}
\newcommand{\UserUtt}[1]{\textit{#1}}
\newcommand{\AgentUtt}[1]{\textit{#1}}
\newcommand{\AgentAction}[1]{\texttt{\small #1}}
\newcommand{\MetaAction}[1]{\texttt{\small \underline{#1}}}
\newcommand{\Exec}{\MetaAction{\textcolor{red}{We don't use eval anymore!}}\xspace}
\newcommand{\Salient}{\MetaAction{refer}\xspace}
\newcommand{\Revise}{\MetaAction{revise}\xspace}
\newcommand{\ReviseConstraint}{\MetaAction{reviseConstraint}\xspace}
\newcommand{\Describe}{\MetaAction{describe}\xspace}
\newcommand{\scExec}{\texttt{\scriptsize\underline{eval}}\xspace}
\newcommand{\scSalient}{\texttt{\scriptsize\underline{refer}}\xspace}
\newcommand{\scRevise}{\texttt{\scriptsize\underline{revise}}\xspace}
\newcommand{\Recover}{\MetaAction{recover}\xspace}
\newcommand{\Root}{\textit{root}}
\newcommand{\OurDataset}{SMCalFlow\xspace}


\newcommand{\justin}[1]{{{\textcolor{purple}{(Justin: #1)}}}}
\newcommand{\derek}[1]{{{\textcolor{blue}{(Derek: #1)}}}}
\newcommand{\wenting}[1]{{{\textcolor{orange}{(Wenting: #1)}}}}
\newcommand{\daniel}[1]{{{\textcolor{brown}{(Daniel: #1)}}}}
\newcommand{\srush}[1]{{{\textcolor{green}{(Sasha: #1)}}}}
\newcommand{\celine}[1]{{{\textcolor{blue}{(Celine: #1)}}}}

\AtBeginDocument{\def\sectionautorefname{Section}}%
\AtBeginDocument{\def\subsectionautorefname{Section}}%


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{
Symbolic Planning and Code Generation
for Grounded Dialogue
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
Justin T. Chiu  \\
Cornell Tech \\
\texttt{jtc257@cornell.edu}
\And
Wenting Zhao \\
Cornell University \\
\texttt{wz346@cornell.edu}
\And
Derek Chen \\
Columbia University \\
\texttt{dc3761@columbia.edu}
\AND
Saujas Vaduguru \\
Carnegie Mellon University \\
\texttt{svadugur@andrew.cmu.edu}
\And
Alexander M. Rush \\
Cornell Tech \\
\texttt{arush@cornell.edu}
\And
Daniel Fried \\
Carnegie Mellon University \\
\texttt{dfried@cs.cmu.edu}
}

\begin{document}
\maketitle
\begin{abstract}
Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding \textsc{OneCommon} dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56\% to 69\% in the most challenging setting.
\end{abstract}

\section{Introduction}
% Dialogue needs planning
Success in grounded task-oriented dialogue requires intentional communication guided by strategic planning \citep[\emph{inter alia}]{cohen1979-speech-acts,traum1994computational,Walker2004-generation,Rieser2009-planning,cicero}. 
Dialogue agents must read partner utterances, update their beliefs, then make a plan that furthers their goal.
These plans must take into account both dialogue history and grounding, such as in an image.
In end-to-end systems based solely on large language models (LLMs),
this process is implicit and therefore difficult to control,
requiring extra supervision \citep{rlhf} or expensive search \citep{astaresque} to improve.
While recent work has taken steps to rectify implicit reasoning via
planning in language space, 
where intermediate steps are generated by an LLM~\citep{cot},
there is no guarantee that these approaches result in plans that further task progress.
Additionally, planning in language space is expensive,
requiring inference in an LLM \citep{yarats2017rollout,bamcp}.

\begin{figure}[!t]
\setlength{\abovecaptionskip}{0pt}

\centering

% \scalebox{1.1}{
% \begin{tikzpicture}

% %\filldraw[gray!40] (0,0) circle (.3em);
% %\filldraw[gray!100] (.25,0) circle (.38em);
% %\filldraw[gray!160] (.5,0) circle (.45em);

% \filldraw[gray!160] (-.2,.6) circle (.45em); % 766
% \filldraw[gray!40] (-.7,-.3) circle (.4em); % 51
% \filldraw[gray!40] (.4,.1) circle (.3em); % 52
% \filldraw[gray!100] (.8,-.1) circle (.3em); % 66

% % left
% \filldraw[gray!140] (-1.3,-1.2) circle (.3em); % 13
% \filldraw[gray!100] (-1.6,-1.8) circle (.3em); % 14
% \filldraw[gray!110] (-.2,-1.1) circle (.4em); % 74

% % right
% \filldraw[gray!100] (-1,1.5) circle (.3em); % 28
% \filldraw[gray!100] (-.5,1.6) circle (.38em); % 69
% \filldraw[gray!160] (.8,1.8) circle (.45em); % 26

% % left selects 76
% \draw[blue,thick] (-.2,.6) circle (.7em);
% % right selects 26
% \draw[red,thick] (.8,1.8) circle (.7em);

% \draw[blue,thick] (-.3,-.7) circle (5em);
% \draw[red,thick] (.2,1) circle (5em);
% \end{tikzpicture}
% }

\includegraphics[width=0.3\textwidth]{imgs/Fig1_SPC.pdf}

\vspace{1em}
%\tiny
\small
\begin{tabular}[b]{lp{0.35\textwidth}} %{@{}p@{}p{}{0.3\textwidth}}
\toprule
% {\color{blue} A}:\;&I have one large black dot by itself. Do you have it? \\
% {\color{red} P}:\;&Yes, I do have that. \\
% {\color{blue} A}:\;&Let's pick that one. \\
% {\color{red} P}:\;&<select> {\color{red} red} \\
% {\color{blue} A}:\;&<select> {\color{blue} blue}\\
{\color{red} SPC}:\;&Do you see a pair of dots, where the top left dot is medium-sized and grey and the bottom right dot is medium-sized and dark \\
{\color{blue} Human}:\;&No \\
{\color{red} SPC}:\;&Do you see a pair of dots, where the top dot is small-sized and light and the bottom dot is medium-sized and grey \\
{\color{blue} Human}:\;&Yes I see them. Is there a small grey dot above the small light dot? \\
{\color{red} SPC}:\;&No. Is there a large size and dark color dot to the left and below those? \\
{\color{blue} Human}:\;&Yes and there is a small grey dot below them as well for me. \\
{\color{red} SPC}:\;&Yes. Let's select the small size and light color one. \\
{\color{red} \system{}}:\;&<select> \\ %{\color{red} red} \\
{\color{blue} Human}:\;&<select> \\ % {\color{blue} blue}\\
\bottomrule
\end{tabular}

\vspace{1em}

\caption{
An example grounded dialogue from \textsc{OneCommon}. Our dialogue agent, {\color{red}SPC}, and a human {\color{blue}partner} have different but overlapping circular views of a shared board.
The agent and partner must collaborate through dialogue in order to find and
select a shared dot.
\textsc{OneCommon} demands careful, grounded reasoning.
\vspace{-.8em}
}
\label{fig:oc}
\end{figure}

Rather than implicit or heuristic reasoning, we are interested in explicit reasoning and planning over
symbolic actions.
Symbolic actions are controllable by construction,
allowing system designers to easily build in task-specific knowledge \citep{he2018dnd,cicero}.
This controllability is crucial for obtaining task-specific success using general tools,
even with LLMs.
% probably need examples here
%\justin{add in code}

We provide an example from \textsc{OneCommon}, 
a particularly challenging grounded dialogue game \citep{onecommon}.
The goal of \textsc{OneCommon} is to, through dialogue, identify one dot in common with your partner,
who has an overlapping but different view of an underlying set of dots, illustrated in \autoref{fig:oc}.
The challenge in \textsc{OneCommon} is grounding the contextual spatial relationships described in language to dots.

Recent work has utilized code-generation for grounded language understanding \citep{vipergpt}.
In particular, they translate natural language questions to code as an intermediate representation, then execute that code to obtain an answer.
Code has a couple appealing properties as an intermediate representation: First, 
modern language models are trained on a mixture of code and natural language, affording them the capability of, with some accuracy, translating between the two \citep{chen2021evaluating}.
Second, code acts as a compositional knowledge representation.
This allows code-generation systems to perform grounded compositional reasoning,
provided a library of Python functions that perform grounding \citep{codeaspolicies2022}.

We present a system, Symbolic Planning and Code-generation (\system{}), 
that \textit{reads} by translating partner utterances into code and \textit{plans}
based on symbolic reasoning over what to say next.
Code as a compositional knowledge representation closely mirrors the compositional nature of utterances, which are composed of grounded parts.
\system{} plans by optimizing expected information gain, which has been shown to be effective at building 
a key aspect of collaborative dialogue: common ground \citep{yu2019info,white-etal-2021-open,ocp}.
Symbolic planning allows \system{} to explicitly and efficiently optimize for task success while taking advantage of task-specific properties.

We evaluate our \system{} system on the most challenging subset of the \textsc{OneCommon} task, comparing our system to the previous state-of-the-art supervised system for the task~\citep{fried}. In both evaluations with human partners and automated self-play evaluations, we find that our approach substantially outperforms the previous state-of-the-art in task accuracy, improving from 56\% to 69\% accuracy, and obtains comparable task accuracy to human-human pairs on average.

%\celine{to be clear, what are the contributions? (1) the sysname method, (2) evaluation of sysname method (...?)}

\section{Related Work}
Prior work on collaborative reference games focuses on building common ground
\citep{mf,pb,pip}.
Prior work by \citet{fried} implements an approximation of pragmatic reasoning on \textsc{OneCommon}, but plans in language space and utilizes supervised models for mapping language to symbols.
\citet{pip} plan in symbolic space, but without natural language.
We plan in symbolic space and map from language to symbols via code generation.

Dialogue systems have a long history of reasoning with symbolic actions.
When available, symbolic actions have been found to improve
the performance of dialogue systems, especially in the setting of grounded dialogue \citep{winograd,young2006pomdp,he2018dnd,sm,cicero}.
The closest work to ours is \textsc{Cicero}, which utilizes symbolic planning in a system for \textsc{Diplomacy},
a dialogue and strategy game that requires negotiation and coordination between players \citep{cicero}.
\textsc{Cicero} requires a supervised dataset to train their system.
We use code LLMs which require minimal supervision beyond constructing a small perceptual grounding API.

Planning in dialogue systems has recently eschewed symbolic actions in favor of
planning directly in text, 
where systems either perform roll-outs, tree-search, or other forms of intermediate reasoning in language.
This allows system designers to avoid manually defining symbolic actions \citep{yarats2017rollout,jang2020bapomdp,gandhi2023strategic}.
However, the accuracy of language-space planners is still low in many settings \citep{fried,valmeekam2023planning}.
We focus on symbolic planning, where planning is defined in a space that ensures accuracy and controllability.

With the recent progress in large language modeling,
code generation for modular grounded systems has quickly gained interest.
Grounded code generation systems do not require task-specific training data, 
making them cheap to apply.
A body of work utilizes a large language model for instruction following
by generating Python code that makes calls to lower-level perception libraries
\citep{codeaspolicies2022,vipergpt,gupta2022visual,gao2023pal}.
This extends prior work on executable semantic parsing \citep{liang2016learning,johnson2017inferring,cheng2018learning} with large language models.
Concurrent work has also utilized code-generation to interpret language, integrated with symbolic reasoning \citep{wong2023word}.
We apply these advances to the setting of grounded task-oriented dialogue,
where code generation grounds language to symbolic actions for use in explicit planning.

\section{Overview: Reference Games}
Collaborative reference games
pair an agent and a partner in order to build common ground through natural language dialogue \citep{pb,pip,mf,onecommon}.
Mirroring realistic scenarios, many reference games are also partially observable,
where the agent and partner have different perspectives, and so they must resolve ambiguity.

\textsc{OneCommon}~\citep{onecommon}, as shown in Figure \ref{fig:oc}, is a reference game that exemplifies two challenges: grounding and planning.
In \textsc{OneCommon}, the agent and partner see different but overlapping views of a set of dots, and the goal is to find and select one dot common to both players' views.
Grounding in \textsc{OneCommon} is particularly difficult due to the dot-based visual context, which requires abstract spatial reasoning.
Planning is complicated by the partial observability caused by differing perspectives, which require agents to use complex referring expressions in order to avoid ambiguity.\footnote{The contexts in \textsc{OneCommon} were constructed to make referring expressions challenging and context-dependent. For example, if the agent sees only light dots, a relatively `dark' dot for the agent may not be considered dark at all by the partner.\textsc{OneCommon} is an ideal testbed for pragmatic methods that reason about contextual meaning. While our approach does not address pragmatics, we hope future work will.}
We focus on \textsc{OneCommon} due to its simplicity and difficulty.

Our approach to grounded reference games separates symbolic reasoning from language, allowing explicit steering.
Our system, Symbolic Planning and Code-generation (\system{}), breaks down a turn into three procedures: reading, planning, and writing.
Reading and writing convert from language to symbols and vice versa,
while planning reasons in purely symbolic space.

The agent maintains a belief distribution over possible worlds, $z$, representing task-specific unknowns.
The goal of dialogue is to gain information about $z$ until the agent is confident enough to end the game.
At each turn, the agent \textbf{reads} the partner's utterance $u$, converting it into a symbolic action, $p(x|u)$. This symbolic action potentially builds upon the action $x'$ of a previous utterance, $u'$.
The agent then \textbf{plans} in symbolic space.
The system uses reasoning to update its belief state, $p(z|u)=\sum_x p(z|x)p(x|u)$,
then produces a response $y^*$ of what to say next, which it describes in language to the partner. There is additionally a templated write module for generating a response from $y^*$ described in Appendix~\ref{sec:templates}.

In \textsc{OneCommon}, given a set of dots $\mcD$, the state $z \in \{0, 1\}^{|\mcD|}$ represents which dots the agent believes are contained (1) and not contained (0) in the partner's view, illustrated in Figure \ref{fig:system}. We call a set of dots a \textit{configuration}.
The action representation of partner, $x$ and $x'$, and agent utterances, $y^*$, alike is also a configuration in $\{0,1\}^{|\mcD|}$, as well as any answers or confirmations to previous questions.

\section{Reading: From Language to Symbols}
\label{sec:reading}
Reading in \system{} requires interpreting utterances to a grounded symbolic action, which in turn facilitates the planning stage. Consider the following exchange:
\vspace{-0.5em}

\begin{Dialogue}
    \AgentSay{Do you see a triangle of dark dots?}
    \Partner{Yes, is there a small grey one below it?}
\end{Dialogue}%
\vspace{-1em}

Reading has several challenges. First, reading requires grounding utterances in context, e.g. the shapes and relations. Second, utterances are compositional. For example, the partner utterance builds on top of the previous utterance through coreference.
Finally, a reading system must act quickly, as real-time dialogue systems require reasonable response times.

\subsection{Code Generation}
\label{sec:code-generation}

In \system{}, reading is implemented as code generation. Given a dialogue, we generate Python code\footnote{We target Python as our code representation since it is well-understood by large language models. However, in principle, our system could target other languages such as Prolog or SQL.} which is then used as a meaning function to produce a distribution over all valid interpretations of the utterance's symbolic action (\autoref{fig:codegen}).
The code calls perceptual library functions with grounded semantics, drawn from a task-specific API.
This perceptual library allows the system to both ground elements of the utterance and compositionally build upon previous utterances. Consider the following abbreviated example, based on \textsc{OneCommon}:

\begin{Dialogue}
    \AgentDo{%
    from perceptual\_library import is\_small, ...
    \newline dot1, dot2, dot3, ... = get\_dots()}
    \AgentSay{Do you see a triangle of dark dots?}
    \AgentDo{agent\_configs = set([\newline
    \strut~~Config(dot1, dot2, dot3), \newline
    \strut~~Config(dot3, dot4, dot1)\newline
    ])}
    \Partner{Yes, is there a small grey one below it?}
    \AgentDo{%
      def turn(prev\_configs):\newline
      \strut~~configs = set()\newline
      \strut~~for prev\_config in prev\_configs: \newline
      \strut~~~~for dot in single\_dots(exclude=prev\_config):\newline
      \strut~~~~~~if (\newline
      \strut~~~~~~~~is\_small(dot)\newline
      \strut~~~~~~~~and is\_grey(dot)\newline
      \strut~~~~~~~~and is\_below(dot, prev\_config)\newline
      \strut~~~~~~):\newline
      \strut~~~~~~~~configs.add(Config(dot, prev\_config))\newline
      \strut~~return configs\newline
      partner\_configs\_x = turn(agent\_configs)
    }
\end{Dialogue}%

\noindent
The code in the meaning function is imperative, but represents a set of declarative constraints representing $p(x|u)$.\footnote{In \textsc{OneCommon}, the distribution over symbolic actions $p(x|u)$ is represented as represented as a categorical distribution over configurations with probabilities based on the size of the circumcircle.}
The meaning function for the partner turn, \texttt{turn(prev\_configs)},
takes as input the distribution over symbolic actions of a previous turn, $p(x')$, and yields a set of possible interpretations of the current turn, $p(x|u) = \sum_{x'}p(x | u,x')p(x')$.\footnote{The symbolic action of a previous turn $x'$ may also depend on other previous utterances $u'$. For simplicity, we omit that in the notation.}
Because utterances can have multiple valid interpretations due to ambiguity, \texttt{prev\_configs} represents a distribution.\footnote{\system{} is able to intentionally produce ambiguous descriptions if that improves task success, as illustrated in this example.}

Within \texttt{turn}, we consider all valid configurations while marginalizing over $x'$, i.e. interpretations in \texttt{prev\_configs}.
For each interpretation, each \texttt{dot} is considered.
If the new \texttt{dot} satisfies the semantics of the utterance, checked step-by-step via grounded perceptual library functions such as \texttt{is\_small(d)}, then it is a valid interpretation of the current utterance and is used to create a new \texttt{Config}.

The perceptual library functions are drawn from a manually-defined library.
For \textsc{OneCommon}, we define these functions using domain-specific knowledge:

\begin{Dialogue}
\AgentDo{
def is\_small(d): return d.size < -0.3
}
\end{Dialogue}
\vspace{-1em}

\noindent The perceptual library for \textsc{OneCommon} can be found \href{https://github.com/justinchiu/onecommon-gpt/tree/main/oc/fns}{here}.

\begin{figure}
\vspace{0.39em}
\includegraphics[width=\columnwidth]{imgs/codegen.pdf}
\caption{
\label{fig:codegen}
Overview of Reading.
The generated meaning function for utterance $u$ takes the previous symbolic action distribution $p(x')$ from a prior turn and yields the interpretations $p(x|u)$, using code as a compositional representation (\autoref{sec:reading}).
\vspace{-1em}
}
\end{figure}

\subsection{Prompting}
\label{sec:prompting}

Reading is implemented with large language model (LLM) code generation.
While LLMs can generate accurate code, full code specifications (\autoref{sec:code-generation}) are lengthy and therefore too slow to generate for real-time use. We break down code generation into four steps, where some steps do not require any calls to an LLM.
Decreasing the number of output tokens guarantees a speedup, assuming consistent latency.
See \href{https://github.com/justinchiu/onecommon-gpt}{the code} for details on the code LLM and prompts we use.\footnote{
We release the code \href{https://github.com/justinchiu/onecommon-gpt}{here}.
}
% \daniel{forward ref to wherever we talk about details of model and prompts?}

\noindent \textit{Dialogue Act:} Classify partner utterances as one of three dialogue acts: Start a \textsc{new} line of questioning, ask a \textsc{follow-up} question, \textsc{end} the dialogue.

\noindent \textit{Reference:} Predict which previous turn $x'$ the utterance is following up on, if any:

\begin{Dialogue}
    \AgentSay{Do you see a triangle?}
    \Partner{Yes, is there a small grey dot below it?}
    \AgentDo{%
        dialogue act: \textsc{follow-up}\newline
        refer: turn 1
    }
\end{Dialogue}%

\noindent The system grounds the dots mentioned in the previous turn: \texttt{agent\_configs}, which is stored by the system. This allows referring to other turns besides the previous.

\noindent \textit{Constraint Generation:} Predict the new dots mentioned in the partner utterance alongside code fragments that express the semantics, without the boilerplate code, in the example above:

\begin{Dialogue}
    \Partner{Yes, is there a small grey one below it?}
    \AgentDo{%
      1 new dot\newline
      is\_small(dot)\newline
      is\_grey(dot)\newline
      is\_below(dot, prev\_dots)
    }
\end{Dialogue}%

\noindent \textit{Compose:} Finally, we utilize a template to compose all of this information back into the full code representation for execution.

\begin{figure*}[t]
\centering
\includegraphics[width=2\columnwidth]{imgs/system-diagram.pdf}
\caption{
\label{fig:system}
Overview of Planning.
Partner utterances are interpreted by a meaning function generated by a code LLM (read), producing a distribution over valid symbolic interpretations, $p(x|u)$.
This is used to symbolically update the belief state, $p(z|u)$, increasing the probability of worlds (shared dots) that are consistent with $x$.
This belief state is used to symbolically plan the agent's next utterance, $y^*$, by optimizing the expected information gain, which is described to the partner (write).
% \vspace{-1em}
}			
\end{figure*}

\section{Planning: From Symbols to Responses}

To perform well in collaborative reference games, it is essential to build common ground quickly and accurately by carefully reasoning about what information has been gathered so far, as well as what to say next.
\system{} addresses these desiderata by planning in symbolic space, over the symbolic actions produced by reading.

We have two challenges: First, to incorporate the new information from the partner's utterance while accounting for task-specific grounding as well as dialogue history. Second, given this new information, the system must decide either to end the game or how to improve the probability of success.

Planning requires us to model the actions of the partner given the shared state. To do this we need task specific models of our partner, $p(x \mid z)$, and our partner's reponse to us, $p(x | z, y)$.  In \textsc{OneCommon}, we model both of these by a heuristic function considering set overlap and dot proximity, described in Appendix~\ref{sec:parameterization}.

\subsection{Belief update}
Starting from a prior over the previous belief $p(z)$,
% we aim to incorporate the probabilistic symbolic information, $p(x|u)$, obtained by reading the partner's utterance.
we incorporate probabilistic evidence from the utterance $p(x|u)$. This requires marginalizing over all valid symbolic actions $x$ from the reading step.
In practice, $p(x|u)$ is sparse, and symbols $x$ with non-zero support are very similar.
We therefore approximate this marginalization with a point estimate:
\begin{equation}
\label{eqn:update}
\begin{aligned}
p(z|u) &= \sum_x p(z|x)p(x|u) \\
&= \sum_x \frac{p(x|z)p(z)}{p(x)} p(x|u) \\
&\approx \sum_x \frac{p(x|z)p(z)}{p(x)} 1(x = x^*)\\
&\propto p(x^* \mid z)p(z),
\end{aligned}
\end{equation}
where $x^* = \argmax_x p(x|u)$.

We give an example of this process in \autoref{fig:system}.
In this case, a `big light dot next to a big dark one' could have two valid interpretations, the big light dot and the black dot to the left, or the other black dot to the right.
We approximate this distribution with the most likely interpretation $x^*$.
In \textsc{OneCommon}, we use the most compact\footnote{We define the compactness of a configuration as the radius of the circumcircle. An ideal approximation would take into account more context, such as the relative sizes.} as $x^*$, yielding the black dot on the left.
The belief state is then updated to $p(z|u)$, shown in \autoref{fig:system} (center).

\subsection{Planning}
Given the updated belief, \system{} then plans its next action.
The challenge here is to ensure task success, e.g. finding one dot in common. This requires both exploring by building common ground, then exploiting that knowledge to win the game.

We formalize exploration as the expected information gain, a quantity that codifies how much the agent can expect to learn about possible worlds $z$ after taking an action \citep{lindley}.
That action then elicits a response from the partner, providing information about the uncertain world state.
For example, if the agent has asked about a set of dots and already received a `no', then asking further questions about those dots would not reduce uncertainty.

Formally, we optimize
\begin{equation}
\label{eqn:eig}
y^* = \argmax_y H[z|u] - \Es{x_y \mid y}{H[z \mid u,y,x_y]},
\end{equation}
where $H[z|u]$ is the entropy of the current belief\footnote{The belief entropy $H[z|u]$ in the definition of information gain is constant with respect to the plan $x$, and can be dropped from the objective.}
and $H[z\mid u,y,x_y]$ the  entropy of the posterior distribution. 
This second term is the key part of the objective. Assuming that we take action $y$, the expectation considers all hypothetical future partner responses $x_y$. We are penalized if after seeing these responses, we are still uncertain about the common ground $z$. This objective therefore encourages actions that reduce uncertainty. \footnote{
The distribution $p(x_y|y)=\sum_z p(x_y|y,z)p(z)$
also uses the partner response model $p(x_y|y,z)$.
}

\system{} chooses to exploit and end the game with the following heuristic:
If the system is confident in success, i.e.\ the probability of task success is greater than hyperparameter $\theta$ (set to $0.8$), \system{} ends the game.

\section{Experimental Setup}
\label{sec:experimental-setup}
We conduct two evaluations of \system{} on the \textsc{OneCommon} task. We compare to the state-of-the-art baseline system of \citet{fried}, which we refer to as Imitate.
Imitate is a pipelined system, where each part is fully supervised. Imitate uses a neural representation of dialogue history in combination with a neural-CRF reference resolution module to understand grounded language. In order to generate, Imitate relies on a pragmatic planning procedure, which plans in a mixture of symbolic and language space, prioritizing descriptions of dots that are easily understood.

We first perform human evaluation, evaluating the task success of systems when paired with human partners. This setting is challenging, requiring the system to handle both the linguistically diverse utterances and a range of strategies of human partners.
We recruit 19 workers from Amazon's Mechanical Turk to play with one of three partners: \system{}, the most successful past system for the task \citep{fried}, or another human.
We pay \$15 per hour, with \$1.00 per game at an average of 4 minutes per game. We additionally give a bonus of \$0.15 for every game.
We use 100 visual contexts from the most difficult\footnote{The number of shared dots is four.} partition of \textsc{OneCommon}.
We pay workers $\$1.00$ per game, with a $\$0.15$ bonus if they win.
We collect 287 completed dialogues in total, where both players selected a dot.

We secondarily evaluate systems in self-play, where systems are paired with a copy of themselves. This isolates strategic efficiency by ensuring the agent's partner has the same skill as the agent. The 200 games share the same contexts across systems.

We include an additional system in self-play, GPT4 2-shot\footnote{We do not include GPT4 2-shot in human evaluation, as its self-play evaluation is very poor.}, which gets two full human dialogues as examples. Each human dialogue example starts with a description of the context the agent sees.
The full prompts can be viewed \href{https://github.com/justinchiu/onecommon-gpt}{here}.

% \paragraph{Hyperparameters}
% Throughout, \system{} sets the probability of success hyperparameter $\theta = 0.8$.

\paragraph{Parameterization}
For code generation, during the reading phase we use GPT-4\footnote{Specifically \texttt{gpt-4-0613}.} \citep{gpt4}.
The symbolic actions in \textsc{OneCommon} consist of sets of dots and confirmations, while the belief over symbolic states, $p(z)$,
captures which dot configurations are shared and is designed to account for dot proximity.
Further details on the prior are given in Appendix \ref{sec:parameterization}.
The symbolic partner models, $p(x\mid z)$ and $p(x\mid y,z)$, are drawn from \citet{ocp}, and incorporate a similar bias based on dot proximity.

\section{Results}

\paragraph{Human evaluation}
In human evaluation,
\system{} obtains substantially higher task accuracy than the baseline model of \citet{fried},
and is comparable to human performance on average. This demonstrates that the combination of symbolic information-gain planning and code-generation in \system{} is more effective than the baseline's language-space planning objective and supervised reference resolution.

We see a more nuanced story when conducting a skill-based analysis of the human evaluation results, presented in Figure \ref{fig:segmented_success}.
A worker's skill is given by their average success rate with other human partners.
The x-axis of the graph, the minimum success rate, increasingly filters workers from left to right:
the left side of the graph shows all workers, while the far right shows only those workers who won nearly all of their human-human games.
Skilled human partners have a higher success rate with other humans, as opposed to when partnered with \system{}.
Additionally, the success rate of \system{} improves with human skill, while the success rate of human partners with the baseline system, Imitate, remains relatively constant across skill levels, implying that \system{} is more responsive than the baseline to strategies used by humans.

\begin{table}[!t]
\centering
\begin{tabular}{lrrrrr}
\toprule
Agent                   & Success & Turns & Games\\
\midrule
\system{}               & 68.8\%  & 7.77 & 96\\
Imitate                 & 55.6\%  & 6.61 & 117\\
Human                   & 67.6\%  & 5.03 & 74\\
\hline
Human$^\dagger$         & 65.8\%  & 4.97 & 2,189\\
\bottomrule
\end{tabular}
\caption{\label{tbl:human-eval}
The average success rate, average number of turns, and total number of games between agents and human partners on the hardest setting of \textsc{OneCommon}, with 4 shared dots.
$^\dagger$ indicates statistics from the \textsc{OneCommon} dataset \citep{onecommon}.
\vspace{1em}
}
\end{table}

\begin{figure}[!t]
\includegraphics[width=\columnwidth]{imgs/Figure_1.pdf}
\caption{
\label{fig:segmented_success}
Success rate of the different agent types with human partners, with progressive filtering of human partners by their success rate along the x-axis. Shaded regions give standard errors.
% \vspace{-1em}
}
\end{figure}


\system{} also takes more turns on average than both the baseline and human-human games.
We hypothesize that this difference is caused by shorter human partner
responses to the system, and therefore less information shared by the human partner.
In Table \ref{tbl:uttlen}, we confirm that the average and median number of words per human utterance are significantly lower for humans partnered with \system{} than any other agent type.


\begin{table}[!t]
\centering
\begin{tabular}{lrr}
\toprule
Agent                    & Avg $|u|$ & Median $|u|$\\
\midrule
\system{}                & 6.95    & 4\\
Imitate                  & 9.62    & 8\\
Human                    & 15.06   & 14\\
\bottomrule
\end{tabular}
\caption{\label{tbl:uttlen}
The average and median number of words per utterance by human partners for different agent types in human evaluation.
\vspace{1em}
}\end{table}

\paragraph{Self-play}
Similarly to human evaluation,
\system{} outperforms the baseline Imitate system in self-play as shown in Table~\ref{tbl:self-play}.
Compared to the baseline, \system{} takes more turns on average, but
has a higher success rate.
We attribute both the longer games and higher success to symbolic planning, which ensures conservative playing.
Interestingly, \system{} self-play takes fewer turns on average than \system-human pairings.
We hypothesize that this is due to both copies of \system{} communicating a consistent amount of information every turn.
This also highlights the importance of human evaluation, which evaluates with a large population of partners.

We also find that GPT4 2-shot performs poorly in self-play. We attribute this to overly-agreeable responses, where the agents choose a dot without thorough verification or reasoning. This occurs despite the much longer dialogues, in comparison to all other agent types.

\begin{table}[!t]
\centering
\begin{tabular}{lrr}
\toprule
Agent                   & Success & Avg \# turns\\
\midrule
\system{}$$             & 84.0\%  & 4.83\\
Imitate                 & 63.5\%  & 3.31\\
GPT4 2-shot             & 19.0\%    & 9.26\\
%ViperGPT               &         & \\
\hline
Human$^\dagger$         & 65.8\%  & 4.97\\
\bottomrule
\end{tabular}
\caption{\label{tbl:self-play}
The success rate of different agents in 200 self-play games on the hardest setting of \textsc{OneCommon}, with 4 shared dots.
A higher success rate is better.
The human performance is from the \textsc{OneCommon} dataset
\citep{onecommon}.
}
\end{table}


\section{Analysis}

\begin{figure}[t]
\setlength{\abovecaptionskip}{0pt}

\centering

% \scalebox{1.1}{
% \begin{tikzpicture}

% \filldraw[gray!160] (-.2,0) circle (.45em); % 766
% \filldraw[gray!40] (-.1,.1) circle (.4em); % 51
% \filldraw[gray!40] (.4,.1) circle (.3em); % 52
% \filldraw[gray!100] (.2,-.1) circle (.3em); % 66

% % left
% \filldraw[gray!140] (-1.3,-.2) circle (.3em); % 13
% \filldraw[gray!100] (-1.6,-.1) circle (.3em); % 14
% \filldraw[gray!110] (-1,-.3) circle (.4em); % 74

% % right
% \filldraw[gray!100] (-1,1.5) circle (.3em); % 28
% \filldraw[gray!100] (-.5,1.6) circle (.38em); % 69
% \filldr;aw[gray!160] (.8,1.8) circle (.45em); % 26

% % left selects 76
% \draw[blue,thick] (.2,.6) circle (.7em);
% % right selects 26
% \draw[red,thick] (.5,1.8) circle (.7em);

% \draw[blue,thick] (.6,.3) circle (5em);
% \draw[red,thick] (.8,-.2) circle (5em);
% \end{tikzpicture}
% }

\includegraphics[width=0.4\textwidth]{imgs/Fig2_SPC.pdf}

\vspace{1em}

\small
\begin{tabular}[b]{@{}l@{}l@{}}
\toprule
{\color{blue} \system{}}:\;& Do you see a pair of dots, where the bottom dot \\
&is medium-sized and dark and the top dot is \\
&small-sized and light\\
{\color{red} Human}:\;& No. Do you see a pair of medium sized dots, \\
&close together, one is dark grey the other\\
&light grey. The light grey one is slightly above \\
&and the left of the dark one.\\  
{\color{blue} \system{}}:\;& No. Do you see a pair of dots, where the left dot\\
&is medium-sized and dark and the right dot is \\
&medium-sized and light\\
{\color{red} Human}:\;& No, do you see a lone medium sized grey dot?\\
{\color{blue} \system{}}:\;& No. Do you see a pair of dots, where the left dot \\
&is medium-sized and light and the right dot is \\
&small-sized and grey\\
{\color{red} Human}:\;& No. do you see a pair where the right one is \\
&medium and grey and the left one is smaller and \\
&lighter. The smaller one is slightly below the \\
&medium sized one.\\          
{\color{blue} \system{}}:\;& Yes. Is there a medium size and light color dot \\
&to the right and below those?\\
{\color{red} Human}:\;& Yes\\
{\color{blue} \system{}}:\;& Let's select the medium size and grey color one.\\
{\color{red} Human}:\;&<select> \\ %{\color{red} red} \\
{\color{blue} \system{}}:\;&<select> \\ % {\color{blue} blue}\\
\bottomrule
\end{tabular}

\vspace{1em}

\caption{
An example dialogue between \system{} and a human partner.
%S_03oXQehOdOfAEbCD
% URGENT: UPDATE IMAGE
\vspace{1em}
}
\label{fig:ex_gpt}
\end{figure}

\paragraph{Qualitative analysis}
We present a qualitative example of a dialogue between \system{} and a human partner in Figure \ref{fig:ex_gpt}.
This long dialogue illustrates the benefits of explicit belief updates and planning:
The conversation starts off with many rejections, which the agent uses to deduce the shared configurations.
Eventually, a common set of dots is found and the game ends in a success.
Without explicit planning, it would have been unlikely for \system{} to have  succeeded at the end of the conversation.
%We provide a contrasting example of a dialogue between the baseline system and a human partner, where the baseline system has difficulty keeping track of the dialogue in Appendix \ref{sec:baseline-example}.
%We visualize the belief state in Figure TODO.

\paragraph{Reading speed analysis}
We perform a speed ablation of the code-generation prompt in \system{}.
\system{} uses a sequence of steps for reading,
involving dialogue act classification, code fragment generation,
and composing the full code representation based on the output of these steps.
We compare this to a prompt that generates the full meaning function.

We evaluate both of these prompts in a reading task,
where the goal is to read utterances generated by \system{} and recover the underlying plans, measured by accuracy.
In Table \ref{tbl:prompt}, we see that both styles of prompts have similar similar accuracy, but the sequential, decomposed approach is much faster due to shorter outputs.

\begin{table}[!t]
\centering
\begin{tabular}{lrrr}
\toprule
Prompt style                   & Acc     & Time (s) & Len\\
\midrule
\system{}                      & 86.7\%  & 5     &  36\\
Full                           & 84.0\%  & 18    &  176\\
\bottomrule
\end{tabular}
\caption{\label{tbl:prompt}
The average accuracy, speed, and output length (number of tokens) for the sequential and full code generation methods in our benchmark reading task.
\vspace{-1.3em}
}
\end{table}

\section{Conclusion}
We present Symbolic Planning and Code-generation (\system{}), a method that approaches grounded task-oriented dialogue by separating symbolic reasoning from language.  Our approach uses an LLM to generate executable code functions which represent the meaning of utterances, mapping from language to symbolic actions. We then symbolically track task progress using Bayesian reasoning,  and explicitly plan the best actions to take next using an information gain objective. 
Despite using minimal supervision, beyond a task-specific API and few-shot examples, our approach substantially outperforms the state-of-the-art system for the \textsc{OneCommon} task in both human evaluations and automatic self-play evaluations.

Our work contrasts with recent work on planning in language space, which reasons implicitly \citep{cot,yarats2017rollout,bamcp}.
While less flexible than language reasoning, symbolic reasoning is both interpretable and modular.
Future work should seek to improve the flexibility of symbolic reasoning \citep{wong2023word}.

Our work also represents a first step toward using general-purpose code as a representation for downstream dialogue and interaction tasks. Future work might explore code-based representations that afford more flexible interaction with people, e.g., representing a broader range of user actions, both linguistic and grounded, to construct broadly useful interactive systems. An ideal system would be able to synthesize these representations with minimal manual intervention.

\section*{Acknowledgements}
We thank Vivian Chen, Sanjiban Choudhury, Ge Gao, Omer Gul, Sedrick Keh, Woojeong Kim, Celine Lee, Jack Morris, Chenran Ning, Jacob Sharf, Alane Suhr, Nicholas Tomlin, Anne Wu, and Jiawei Zhou for discussions, game-playing, and feedback at various points in the process.

We also thank the Mechanical Turkers of Turker Nation for their efforts in game-playing.

JC is supported by NSF \#2242302. AMR is supported by a Sloan Fellowship and NSF CAREER \#2037519.
SV and DF were supported by gifts from Google and from Autodesk Research.

\section*{Limitations}
Our system performs code execution given human input,
opening our system to several risks, such as code injection and unauthorized access.
Future work must strive to integrate code execution capabilities in a secure manner.

Our approach also requires the manual engineering of a domain-specific API, as well as a symbolic representation. Future work should seek to alleviate the amount of manual engineering in order to improve flexibility. We hope that methods in program synthesis can provide a solution.

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

\section{Prompt details}
All prompts rely on few-shot prompting. Reformat has 5 few-shot examples, Classify has two dialogues with 15-turns total, Confirm has 9 examples, and Understand has two dialogues with 15 turns total. All examples were based loosely on 10 examples from the human-human games collected in OneCommon by \citet{onecommon}. The same prompts were used in every context.
The full prompts can be found \href{https://github.com/justinchiu/onecommon-gpt}{here}.

\section{Prompt ablation}
We present an additional experiment on how the choice of few-shot examples affects the code constraint generation prompt, which is a key component of the reading step.
The code constraints express the relationships between the mentioned dots, e.g. whether they form a triangle or their relative positions, shapes, and colors.

We take the first human utterance from 20 games in human evaluation and examine whether the parsed answer changes when the prompt examples are changed. The 15 examples in the constraint generation prompt were labeled by hand. Since we cannot sample another 15 examples, we instead sub-sample 5 random examples out of 15 for a 5-shot prompt. We report the average agreement between 5-shot prompts and the original 15-shot prompt across 5 trials: 99\%, with a standard deviation of 2\%. This implies the constraint generation prompt is not sensitive to prompt example choice at the 5-shot level and prompts could be further optimized.

We perform the same experiment with 5 trials of 1-shot prompts and see an average agreement rate of 34\% with a standard deviation of 42\%. This implies that given a single example, the prompt example matters.

We also find that a zero-shot prompt is unable to generate output in the correct format.

\section{Writing}
\label{sec:templates}
We utilize three templates for writing, one for each dialogue act.

\noindent\textsc{start}: \texttt{Do you see a pair of dots, where the \{position\} dot is \{size\}-sized and \{color\} and the \{position\} dot is \{size\}-sized and \{color\}?}

\noindent\textsc{follow-up}: \texttt{Is there a \{size\} size and \{color\} color dot \{position\} those?}

\noindent\textsc{select}: \texttt{Let's select the \{size\} size and \{color\} color one. <selection>}

\section{Parameterization}
\label{sec:parameterization}
We give the parameterization of the belief prior, $p(z)$ for \textsc{OneCommon}.

Our goal in designing the prior is to ensure that the closer dots are, the more likely they are to be of the same state: either all shared or not.
This reflects the contiguity of \textsc{OneCommon} perspectives.

The prior is given by
\begin{equation}
p(z) \propto \exp(f(z)),
\end{equation}
where $f(z)$ is given the sum of the edges of a minimum spanning tree for the dots in $z$. The weights of this spanning tree are determined by the rank of how close the dots are to each other.
The edge between the nearest neighbor of a dot and the dot itself gets assigned a weight of 0, the 2nd nearest neighbor a weight of 1, and so on.

\section{Relation to prior work in semantic parsing and dialogue state tracking}
Prior work in semantic parsing for dialogue state tracking, such as in SMCalFlow \citep{sm}, does not ground in a visual context and also requires strategic, collaborative planning due to OneCommon’s symmetric roles. Agents must both give and request information strategically. This type of strategic reasoning is not explored in prior works in semantic parsing and dialogue state tracking.
Our technical contribution is unifying grounded language understanding and strategic symbolic reasoning with code generation. In particular, the reading phase of SPC was designed for spatial reasoning in OneCommon. 

\end{document}

